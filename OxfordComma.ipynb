{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg, reuters, brown\n",
    "from nltk.tree import Tree\n",
    "import nltk\n",
    "import html\n",
    "import spacy\n",
    "import bllipparser\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "import os\n",
    "import time\n",
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: /Users/kesslej/.local/share/bllipparser/WSJ+Gigaword-v2\n",
      "Model directory already exists, not reinstalling\n"
     ]
    }
   ],
   "source": [
    "rrp = bllipparser.RerankingParser.fetch_and_load('WSJ+Gigaword-v2', verbose=True)\n",
    "nlp = spacy.en.English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/kesslej/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to /Users/kesslej/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/kesslej/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.download('reuters')\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_coordinated_nps(tree):\n",
    "    if isinstance(tree, Tree):\n",
    "        if tree.label() == 'NP':\n",
    "            for coordinates in get_coordinates(tree):\n",
    "                yield coordinates\n",
    "        for subtree in tree:\n",
    "            for coordinates in get_coordinated_nps(subtree):\n",
    "                yield coordinates\n",
    "   \n",
    "def adjust_for_sticky_final_nps(coordinates):\n",
    "    if len(coordinates) > 1:\n",
    "        if coordinates[-1].label() == 'NP' and len(coordinates[-1]) > 2:\n",
    "            # match for NP|NN* (,) CC NP|NN*\n",
    "            if (coordinates[-1][0].label()[:2] in ['NP', 'NN']\n",
    "                and coordinates[-1][-1].label()[:2] in ['NP', 'NN']\n",
    "                and coordinates[-1][-2].label()[:2] == 'CC'\n",
    "                and (len(coordinates[-1]) == 3\n",
    "                     or (len(coordinates[-1]) == 4 and coordinates[-1][1].label()[:2] == ','))):\n",
    "                coordinates = coordinates[:-1] + [coordinates[-1][0], coordinates[-1][-1]]\n",
    "    return coordinates\n",
    "\n",
    "def get_coordinates(tree):\n",
    "    coordinates = []\n",
    "    last_constituent_conjunct = True\n",
    "    for subtree in tree:\n",
    "        if last_constituent_conjunct and subtree.label()[:2] in ('NP', 'NN'):\n",
    "            coordinates.append(subtree)\n",
    "            last_constituent_conjunct = False\n",
    "        elif subtree.label()[:2] in ('CC', ','):\n",
    "            last_constituent_conjunct = True\n",
    "        else:\n",
    "            break\n",
    "    # Common errors: ((John), (Bill and Scott)) or (John, (Bill, and Scott))\n",
    "    # should be ((John), (Bill) and (Scott))\n",
    "    coordinates = adjust_for_sticky_final_nps(coordinates)\n",
    "    if len(coordinates) > 2:\n",
    "        yield coordinates\n",
    "        \n",
    "def get_number_from_np(tree):\n",
    "    pos_list = [pos for orth, pos in tree.pos()]\n",
    "    if pos_list[-1][-1] == 'S' or 'CC' in pos_list:\n",
    "        return 'P'\n",
    "    return 'S'\n",
    "\n",
    "def get_number_from_np(tree):\n",
    "    pos_list = [pos for orth, pos in tree.pos()]\n",
    "    if type(tree[0]) == str:\n",
    "        if tree.label() in ('NNS', 'NNPS'):\n",
    "            return 'P'\n",
    "        else:\n",
    "            return 'S'\n",
    "    if 'CC' in pos_list:\n",
    "        return 'P'\n",
    "    for subsubtree in reversed(tree):\n",
    "        if subsubtree.label() == 'NP':\n",
    "            return get_number_from_np(subsubtree)\n",
    "        else:\n",
    "            if subsubtree.label() in ('NNS', 'NNPS'):\n",
    "                return 'P'\n",
    "            if subsubtree.label() in ('NN', 'NNP'):\n",
    "                return 'S'\n",
    "    return 'S'\n",
    "\n",
    "def which_is_more_ambiguous(numbers):\n",
    "    if numbers[-3:] == ['S', 'S', 'S']: return 'Oxford' # my mother, Jill, and Sam\n",
    "    if numbers[-3:] == ['S', 'S', 'P']: return 'Oxford' # my mother, Jill, and the Smiths\n",
    "    if numbers[-3:] == ['S', 'P', 'S']: return 'Neither' # my mother, the Smiths, and Sam\n",
    "    if numbers[-3:] == ['S', 'P', 'P']: return 'Neither' # my mother, the Smiths, and the Joneses\n",
    "    if numbers[-3:] == ['P', 'S', 'S']: return 'Lack of Oxford' # my parents, Jill and Sam\n",
    "    if numbers[-3:] == ['P', 'P', 'S']: return 'Lack of Oxford' # my family, the Smiths and Sam\n",
    "    if numbers[-3:] == ['P', 'S', 'P']: return 'Lack of Oxford' # my family, Sam and the Smiths\n",
    "    if numbers[-3:] == ['P', 'P', 'P']: return 'Neither' # my family, the Smiths and the Joneses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca01\n",
      "{'nps': 'the widespread interest in the election~~~the number of voters~~~the size of this city', 'sent': \"`` Only a relative handful of such reports was received '', the jury said, `` considering the widespread interest in the election, the number of voters and the size of this city ''.\", 'fileid': 'ca01', 'numbers': ['S', 'S', 'S'], 'ambiguity': 'Oxford'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# patch to get the bllipparser to work with dask\n",
    "'''\n",
    "import dask.bag as db\n",
    "import sys\n",
    "sys.modules['JohnsonReranker'] = bllipparser.JohnsonReranker\n",
    "'''\n",
    "def toks2text(toks):\n",
    "    return (' '.join(toks).replace(\" 's\", \"'s\").replace(' , ', ', ')\n",
    "            .replace(\" .\", '.').replace(' - ', '-').replace('( ', '(').replace(' )', ')'))\n",
    "\n",
    "\n",
    "data = []\n",
    "for fileid in brown.fileids()[:2]:\n",
    "    print(fileid)\n",
    "    #raw_documents_less_headline = '\\n'.join(brown.raw(fileid).split('\\n'))\n",
    "    for sent in brown.sents(fileid):\n",
    "        #print(sent)\n",
    "        reformated_sentence = html.unescape(toks2text(sent))\n",
    "        data.append({'fileid': fileid, 'sent':reformated_sentence})\n",
    "        try:\n",
    "            tree = rrp.parse(reformated_sentence).fuse().as_nltk_tree()\n",
    "        except:\n",
    "            print('bad sentence')\n",
    "            print(reformated_sentence)\n",
    "            continue\n",
    "        #print(reformated_sentence)\n",
    "        for coord in get_coordinated_nps(tree):\n",
    "            nps = '~~~'.join([' '.join(np.leaves()) for np in cÂ®foord])\n",
    "            try:\n",
    "                numbers = [get_number_from_np(np) for np in coord]\n",
    "            except:\n",
    "                import pdb; pdb.set_trace()\n",
    "                continue\n",
    "            ambiguity = which_is_more_ambiguous(numbers)\n",
    "            entry = {'fileid': fileid, \n",
    "                     #'tree': tree,\n",
    "                     'sent': reformated_sentence,\n",
    "                     'nps': nps,\n",
    "                     'numbers': numbers,\n",
    "                     'ambiguity': ambiguity}\n",
    "            print(entry)\n",
    "            data.append(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.to_csv('brown_ambiguity.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Oxford            9\n",
       "Lack of Oxford    4\n",
       "Neither           3\n",
       "Name: ambiguity, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ambiguity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
